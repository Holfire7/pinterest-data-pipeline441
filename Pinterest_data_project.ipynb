{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73d1d26-2cc0-4566-bbf0-20405acfd694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "delta_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "credentials_df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "ACCESS_KEY = credentials_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = credentials_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "AWS_S3_BUCKET = \"user-0eaf46a0829f-bucket\"\n",
    "MOUNT_NAME = \"/mnt/Ben_data_mount\"\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b788a31-a53f-40ae-b06a-7c0341ff064b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "location_of_pinfile = \"s3://user-0eaf46a0829f-bucket/topics/0eaf46a0829f.pin/partition=0/*.json\"\n",
    "\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "\n",
    "df_pin = spark.read.format(file_type).option(\"inferSchema\", infer_schema).load(location_of_pinfile)\n",
    "\n",
    "display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae823b9-3de9-4a73-8ddc-a629169f16fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#This cell here \n",
    "file_location = \"s3://user-0eaf46a0829f-bucket/topics/0eaf46a0829f.geo/partition=0/*.json\"\n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_geo = spark.read.format(file_type).option(\"inferSchema\", infer_schema).load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87afbf14-e827-4fe1-be60-5884bece445a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_geo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43490e2-5d3c-4f87-816e-a60903eb29ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "location_of_userfile = \"s3://user-0eaf46a0829f-bucket/topics/0eaf46a0829f.user/partition=0/*.json\"\n",
    "\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "\n",
    "df_user = spark.read.format(file_type).option(\"inferSchema\", infer_schema).load(location_of_userfile)\n",
    "\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f13e783-4e62-44d0-8981-11db5f7f9a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91cefc68-9598-4dd8-baa6-4c019b2469ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d6a9c1-04a3-4a39-8b25-db068a93e391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "def clean_data(df_pin):\n",
    "    cleaned_df = df_pin.replace({'No description available Story format': None,}, subset=['description'])\n",
    "    cleaned_df = cleaned_df.replace({'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e': None,}, subset=['tag_list'])\n",
    "    cleaned_df = cleaned_df.replace({'No Title Data Available': None,}, subset=['title'])\n",
    "    cleaned_df = cleaned_df.replace({'User Info Error': None, }, subset=['follower_count'])\n",
    "    cleaned_df = cleaned_df.replace({'Image src error.': None, }, subset=['image_src'])\n",
    "    cleaned_df = cleaned_df.replace({'User Info Error': None, }, subset=['poster_name'])\n",
    "\n",
    "    cleaned_data_df = cleaned_df.withColumn('follower_count', cleaned_df['follower_count'].cast('int'))\n",
    "    cleaned_data_df = cleaned_df.withColumn('index', cleaned_df['index'].cast('int'))\n",
    "    cleaned_data_df = cleaned_data_df.withColumn('save_location', regexp_replace('save_location', 'Local save in', ''))\n",
    "    cleaned_data_df = cleaned_data_df.withColumn('follower_count', regexp_replace('follower_count', 'k', '000'))\n",
    "    cleaned_data_df = cleaned_data_df.withColumn('follower_count', regexp_replace('follower_count', 'M', '000000'))\n",
    "\n",
    "    cleaned_data_df = cleaned_data_df.withColumnRenamed('index', 'ind')\n",
    "    cleaned_data_df = cleaned_data_df.select('ind', 'unique_id', 'title', 'description', 'follower_count', 'poster_name', 'tag_list', 'is_image_or_video', 'image_src', 'save_location', 'category')\n",
    "\n",
    "    return cleaned_data_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4deb84-b850-46c9-aaad-974795d50d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_pin = clean_data(df_pin)\n",
    "display(cleaned_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac3b5c7-769c-4e76-a3d4-e5659c66b829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_pin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0972f134-1f10-468e-be3a-4f3438f1985c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col, to_timestamp\n",
    "\n",
    "def cleaned_geo(df_geo):\n",
    "    clean_df = df_geo.withColumn('coordinates', array(col('longitude'), col('latitude')))\n",
    "    clean_df = clean_df.drop('longitude', 'latitude')\n",
    "\n",
    "    clean_df = clean_df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "    clean_df = clean_df.select('ind', 'country', 'coordinates', 'timestamp')\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45892332-948a-40fd-a749-3ee615324b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_geo_df = cleaned_geo(df_geo)\n",
    "display(cleaned_geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5634dd8-5348-4ad4-9687-9132646614e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_geo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c30e88-2bd1-4c3d-b7ce-66950c1dd851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "def cleaned_user(df_user):\n",
    "    clean_user_df = df_user.withColumn('user_name', concat('first_name', 'last_name'))\n",
    "    clean_user_df = clean_user_df.drop('first_name', 'last_name')\n",
    "\n",
    "    clean_user_df = clean_user_df.withColumn('date_joined', to_timestamp(col('date_joined')))\n",
    "    clean_user_df = clean_user_df.select('ind' , 'user_name', 'age', 'date_joined')\n",
    "    return clean_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ec9c00-3ccd-4519-807e-8ef8ad6ba522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_user = cleaned_user(df_user)\n",
    "display(cleaned_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93546b13-fdd3-4643-81fa-029f88ccd3a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_user.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5462850d-2a3d-470f-b982-abc7d876e961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "joined_df = cleaned_pin.join(cleaned_geo_df, cleaned_pin[\"ind\"] == cleaned_geo_df[\"ind\"], how=\"inner\")\n",
    "\n",
    "people_cat = Window.partitionBy(\"category\").orderBy(\"country\")\n",
    "the_result = joined_df.withColumn(\"category_count\", count(\"*\").over(people_cat))\n",
    "\n",
    "the_result = the_result.select(\"category\", \"country\", \"category_count\").distinct()\n",
    "display(the_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ca8113-5ac6-4f54-9043-1715e8d2daf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, year, col, asc\n",
    "\n",
    "\n",
    "joined_df = cleaned_pin.join(cleaned_geo_df, cleaned_pin[\"ind\"] == cleaned_geo_df[\"ind\"], how=\"inner\")\n",
    "years_df = joined_df.withColumn(\"post_year\", year(col(\"timestamp\")))\n",
    "year_cat = years_df.filter(col(\"post_year\").between(2018, 2022))\n",
    "\n",
    "result_df = years_df.groupBy(\"category\", \"post_year\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "display(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69252023-35da-484a-b4ad-4762daaaeb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "joined_df = cleaned_pin.join(cleaned_geo_df, cleaned_pin[\"ind\"] == cleaned_geo_df[\"ind\"], how=\"inner\")\n",
    "\n",
    "user_df = Window.partitionBy(\"country\").orderBy(desc(\"follower_count\"))\n",
    "maxed_df = joined_df.withColumn(\"rank\", row_number().over(user_df))\n",
    "\n",
    "country_rank = maxed_df.filter(maxed_df.rank == 1)\n",
    "result_df = country_rank.select(\"country\", \"poster_name\", \"follower_count\")\n",
    "\n",
    "max_result_df = result_df.orderBy(desc(\"follower_count\")).limit(1)\n",
    "max_results_df = max_result_df.select(\"country\", \"follower_count\")\n",
    "\n",
    "display(max_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdbad65-afe9-4cb3-89eb-5731335de06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "joined_df = cleaned_pin.join(cleaned_user, cleaned_pin[\"ind\"] == cleaned_user[\"ind\"], how=\"inner\")\n",
    "age_df = joined_df.withColumn(\"age_group\", when(col(\"age\").between(18, 24), \"18-24\").when(col(\"age\").between(25, 35), \"25-35\").when(col(\"age\").between(36, 50), \"36-50\").when(col(\"age\")> 50, \"50+\"))\n",
    "age_cat = age_df.groupBy(\"age_group\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "age_cat = age_cat.select(\"age_group\", \"category\", \"category_count\")\n",
    "display(age_cat)\n",
    "# we need to get max category per age_group, so we'll want to use the partition by age_group so that we can then order each category count in descending order filtered by age group.\n",
    "w = Window.partitionBy(\"age_group\").orderBy(desc(\"category_count\"))\n",
    "\n",
    "# We can now use the row_number function to get the first row in each partition, and then filter to only keep the first row\n",
    "age_cat = age_cat.withColumn(\"row\", row_number().over(w)).filter(col(\"row\") == 1).drop(\"row\")\n",
    "display(age_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b10efc4-e1c2-441f-8303-9ed6c9f2aac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, asc, row_number\n",
    "\n",
    "joined_df = cleaned_pin.join(cleaned_user, cleaned_pin[\"ind\"] == cleaned_user[\"ind\"], how=\"inner\")\n",
    "age_df = joined_df.withColumn(\"age_group\", when(col(\"age\").between(18, 24), \"18-24\") \\\n",
    "                              .when(col(\"age\").between(25, 35), \"25-35\") \\\n",
    "                              .when(col(\"age\").between(36, 50), \"36-50\") \\\n",
    "                              .when(col(\"age\")> 50, \"50+\")) \\\n",
    "                              .orderBy(asc(\"age_group\"))\n",
    "median_age = age_df.groupBy(\"age_group\").agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "\n",
    "\n",
    "display(median_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392e3a06-9e32-4d5c-9ba5-07b3bf8e9949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, year, col\n",
    "\n",
    "joined_data = cleaned_geo_df.join(cleaned_user, cleaned_geo_df[\"ind\"] == cleaned_user[\"ind\"], how=\"inner\")\n",
    "\n",
    "user_data = joined_data.withColumn(\"post_year\", year(col(\"timestamp\")))\n",
    "\n",
    "joined_year = user_data.filter(col(\"post_year\").between(2015, 2020))\n",
    "\n",
    "users_by_year = joined_year.groupBy(\"post_year\").agg(countDistinct(\"user_name\").alias(\"number_users_joined\"))\n",
    "\n",
    "display(users_by_year.orderBy(\"post_year\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b616ccb-6857-430a-b4c2-d738d45a4388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, year, col\n",
    "\n",
    "joined_df = cleaned_pin.join(cleaned_user, cleaned_pin[\"ind\"] == cleaned_user[\"ind\"], how=\"inner\")\n",
    "final_join = joined_df.join(cleaned_geo_df, on=\"ind\", how=\"inner\")\n",
    "\n",
    "user_follow = final_join.withColumn(\"post_year\", year(col(\"timestamp\")))\n",
    "user_follow_year = user_follow.filter(col(\"post_year\").between(2015, 2020))\n",
    "\n",
    "user_follow_count = user_follow_year.groupBy(\"post_year\").agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "\n",
    "\n",
    "display(user_follow_count.orderBy(\"post_year\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207fccd2-8399-412c-b3f9-8573b7d40e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "joined_df = cleaned_pin.join(cleaned_user, on=\"ind\", how=\"inner\")\n",
    "final_join = joined_df.join(cleaned_geo_df, on=\"ind\", how=\"inner\")\n",
    "\n",
    "user_date = final_join.withColumn(\"post_year\", year(col(\"timestamp\"))) \\\n",
    "                      .withColumn(\"age_group\", when(col(\"age\").between(18, 24), \"18-24\") \\\n",
    "                                  .when(col(\"age\").between(25, 35), \"25-35\") \\\n",
    "                                  .when(col(\"age\").between(36, 50), \"36-50\") \\\n",
    "                                  .when(col(\"age\")> 50, \"50+\")) \\\n",
    "                                  .orderBy(asc(\"age_group\"))\n",
    "user_date_year = user_date.filter(col(\"post_year\").between(2015, 2020))\n",
    "\n",
    "\n",
    "user_date_count = user_date_year.groupBy(\"age_group\", \"date_joined\").agg(F.expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "user_date_count = user_date_count.select(\"age_group\", \"date_joined\", \"median_follower_count\")\n",
    "\n",
    "display(user_date_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pinterest_data_project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
