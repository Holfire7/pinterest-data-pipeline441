{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c44d048-aef1-4dbf-9d4b-418b1f6e869b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "delta_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "credentials_df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "ACCESS_KEY = credentials_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = credentials_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f102ef9-4e6b-40eb-9b96-334ad6d40b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.formatCheck.enabled=false;\n",
    "SET spark.databricks.kinesis.listShards.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23d5733-0b18-414f-8539-d8dd7fb80e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "stream_pin_df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'Kinesis-Prod-Stream') \\\n",
    "    .option('initialPosition', 'latest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('accessKeyId', ACCESS_KEY) \\\n",
    "    .option('secretAccessKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "pin_schema = StructType([\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"downloaded\", IntegerType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "stream_pin_df = stream_pin_df.filter(stream_pin_df.partitionKey == \"pin-partition\")\n",
    "stream_pin_df = stream_pin_df.selectExpr(\"CAST(data as STRING) jsonData\")\n",
    "stream_pin_df = stream_pin_df.select(from_json(\"jsonData\", pin_schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "display(stream_pin_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5054f9-7d87-4cc9-ba88-228f24a30157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "stream_geo_df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'Kinesis-Prod-Stream') \\\n",
    "    .option('initialPosition', 'latest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('accessKeyId', ACCESS_KEY) \\\n",
    "    .option('secretAccessKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "stream_geo_df = stream_geo_df.filter(stream_geo_df.partitionKey == \"geo-partition\")\n",
    "stream_geo_df = stream_geo_df.selectExpr(\"CAST(data as STRING) jsonData\")\n",
    "stream_geo_df = stream_geo_df.select(from_json(\"jsonData\", geo_schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "display(stream_geo_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f4b9be-e8de-4ec2-b4e5-1af94df52724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "stream_user_df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'Kinesis-Prod-Stream') \\\n",
    "    .option('initialPosition', 'latest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('accessKeyId', ACCESS_KEY) \\\n",
    "    .option('secretAccessKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True),\n",
    "])\n",
    "\n",
    "stream_user_df = stream_user_df.filter(stream_user_df.partitionKey == \"user-partition\")\n",
    "stream_user_df = stream_user_df.selectExpr(\"CAST(data as STRING) jsonData\")\n",
    "stream_user_df = stream_user_df.select(from_json(\"jsonData\", user_schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "display(stream_user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07dd861-6aaa-49b0-af7e-209a77f826e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "def clean_pin_data(stream_pin_df):\n",
    "    pin_cleaned_df = stream_pin_df.replace({'No description available Story format': None,}, subset=['description'])\n",
    "    pin_cleaned_df = pin_cleaned_df.replace({'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e': None,}, subset=['tag_list'])\n",
    "    pin_cleaned_df = pin_cleaned_df.replace({'No Title Data Available': None,}, subset=['title'])\n",
    "    pin_cleaned_df = pin_cleaned_df.replace({'User Info Error': None, }, subset=['follower_count'])\n",
    "    pin_cleaned_df = pin_cleaned_df.replace({'Image src error.': None, }, subset=['image_src'])\n",
    "    pin_cleaned_df = pin_cleaned_df.replace({'User Info Error': None, }, subset=['poster_name'])\n",
    "\n",
    "    cleaned_pin_data_df = pin_cleaned_df.withColumn('follower_count', pin_cleaned_df['follower_count'].cast('int'))\n",
    "    cleaned_pin_data_df = cleaned_pin_data_df.withColumn('save_location', regexp_replace('save_location', 'Local save in', ''))\n",
    "    cleaned_pin_data_df = cleaned_pin_data_df.withColumn('follower_count', regexp_replace('follower_count', 'k', '000'))\n",
    "    cleaned_pin_data_df = cleaned_pin_data_df.withColumn('follower_count', regexp_replace('follower_count', 'M', '000000'))\n",
    "\n",
    "    cleaned_pin_data_df = cleaned_pin_data_df.withColumnRenamed('index', 'ind')\n",
    "    cleaned_pin_data_df = cleaned_pin_data_df.select('ind', 'unique_id', 'title', 'description', 'follower_count', 'poster_name', 'tag_list', 'is_image_or_video', 'image_src', 'save_location', 'category')\n",
    "\n",
    "    return cleaned_pin_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ecbec0-f1ee-4a6e-85b8-0b6556cac9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pin_df_cleaned = clean_pin_data(stream_pin_df)\n",
    "display(pin_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46af700-2218-422d-a499-724a2a08cc3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col, to_timestamp\n",
    "\n",
    "def cleaned_geo_df(stream_geo_df):\n",
    "    clean_geo_df = stream_geo_df.withColumn('coordinates', array(col('longitude'), col('latitude')))\n",
    "    clean_geo_df = clean_geo_df.drop('longitude', 'latitude')\n",
    "\n",
    "    clean_geo_df = clean_geo_df.withColumn('timestamp', to_timestamp(col('timestamp')))\n",
    "    clean_geo_df = clean_geo_df.select('ind', 'country', 'coordinates', 'timestamp')\n",
    "\n",
    "    return clean_geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "186b093d-1b0b-412e-b216-dae71923f317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_geo_df = cleaned_geo_df(stream_geo_df)\n",
    "display(cleaned_geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d7b151-259a-4ed5-9999-a5a1b6ad0c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "def cleaned_user_df(stream_user_df):\n",
    "    clean_user_df = stream_user_df.withColumn('user_name', concat('first_name', 'last_name'))\n",
    "    clean_user_df = clean_user_df.drop('first_name', 'last_name')\n",
    "    clean_user_df = clean_user_df.withColumn('user_name', regexp_replace(\"user_name\", \"([a-z]) ([A-Z])\", r\"\\1 \\2\")) \n",
    "\n",
    "    clean_user_df = clean_user_df.withColumn('date_joined', to_timestamp(col('date_joined')))\n",
    "    clean_user_df = clean_user_df.select('ind' , 'user_name', 'age', 'date_joined')\n",
    "    return clean_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347efeac-3c36-46b5-945a-f55ccfd7af03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_user_stream = cleaned_user_df(stream_user_df)\n",
    "display(cleaned_user_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874a8271-b927-48bc-9991-31a53441e93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)\n",
    "\n",
    "pin_df_cleaned.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "    .table(\"0eaf46a0829f_pin_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75eccb6e-db9b-4611-b23e-a7d40c99e088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cleaned_geo_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/cleaned_geo_df/\") \\\n",
    "    .table(\"0eaf46a0829f_geo_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6524c9ba-7694-426b-80aa-3ff8466faacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cleaned_user_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/cleaned_user_stream/\") \\\n",
    "    .table(\"0eaf46a0829f_user_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3359017282345996,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AWS Kinesis work",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
